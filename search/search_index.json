{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Courses Courses Self-paced learning","title":"Home"},{"location":"#courses","text":"Self-paced learning","title":"Courses"},{"location":"CMU_15213/","text":"15-213/14-513/15-513: Introduction to Computer Systems (cmu.edu) Labs CS:APP3e, Bryant and O'Hallaron (cmu.edu) 15-213/15-513 Intro to Computer Systems: Resources pick an available term and select Schedule on left Lab environment To run with docker (zsh, radare2, etc. installed) : luchaoqi/cmu-15213 - Docker Image | Docker Hub docker-compose run --rm cmu docker run --rm -it -v \"$PWD\":/cmu luchaoqi/cmu-15213 --cap-add=SYS_PTRACE --security-opt seccomp=unconfined zsh Computer Systems: A Programmer's Perspective, 3/E (CS:APP3e) Interactive shell using Docker Compose - Stack Overflow docker-compose up You can set stdin_open: true, tty: true, however that won't actually give you a proper shell with up, because logs are being streamed from all the containers. Resources C bootcamp: CSE 251 Programming in C (msu.edu) Checkout the Handouts session on the top; it gives lots of concise information: CS107 Computer Organization & Systems (stanford.edu) Radare2 for reverse engineering: radareorg/radare2: UNIX-like reverse engineering framework and command-line toolset Videos 2015 Fall: 15-213 Introduction to Computer Systems : Panopto Text books Reading the text book three times makes everything easy CSAPP-Labs/Computer Systems A Programmers Perspective (3rd).pdf at master \u00b7 Sorosliu1029/CSAPP-Labs (github.com) ebook - The C Programming Language Ritchie & kernighan -.doc (archive.org)","title":"CMU 15213"},{"location":"CMU_15213/#labs","text":"CS:APP3e, Bryant and O'Hallaron (cmu.edu) 15-213/15-513 Intro to Computer Systems: Resources pick an available term and select Schedule on left","title":"Labs"},{"location":"CMU_15213/#lab-environment","text":"To run with docker (zsh, radare2, etc. installed) : luchaoqi/cmu-15213 - Docker Image | Docker Hub docker-compose run --rm cmu docker run --rm -it -v \"$PWD\":/cmu luchaoqi/cmu-15213 --cap-add=SYS_PTRACE --security-opt seccomp=unconfined zsh Computer Systems: A Programmer's Perspective, 3/E (CS:APP3e) Interactive shell using Docker Compose - Stack Overflow docker-compose up You can set stdin_open: true, tty: true, however that won't actually give you a proper shell with up, because logs are being streamed from all the containers.","title":"Lab environment"},{"location":"CMU_15213/#resources","text":"C bootcamp: CSE 251 Programming in C (msu.edu) Checkout the Handouts session on the top; it gives lots of concise information: CS107 Computer Organization & Systems (stanford.edu) Radare2 for reverse engineering: radareorg/radare2: UNIX-like reverse engineering framework and command-line toolset","title":"Resources"},{"location":"CMU_15213/#videos","text":"2015 Fall: 15-213 Introduction to Computer Systems : Panopto","title":"Videos"},{"location":"CMU_15213/#text-books","text":"Reading the text book three times makes everything easy CSAPP-Labs/Computer Systems A Programmers Perspective (3rd).pdf at master \u00b7 Sorosliu1029/CSAPP-Labs (github.com) ebook - The C Programming Language Ritchie & kernighan -.doc (archive.org)","title":"Text books"},{"location":"CMU_15213/L0_cprogramminglab/","text":"Resources CSE 251 Programming in C (msu.edu) Takeaways Lean basic C syntax","title":"L0 cprogramminglab"},{"location":"CMU_15213/L0_cprogramminglab/#resources","text":"CSE 251 Programming in C (msu.edu)","title":"Resources"},{"location":"CMU_15213/L0_cprogramminglab/#takeaways","text":"Lean basic C syntax","title":"Takeaways"},{"location":"CMU_15213/L1_datalab/","text":"Resources Bit Twiddling Hacks (stanford.edu) Lecture 16 \u2013 Signed Integers and Integer Arithmetic (umbc.edu) Takeaways Learn common bit ops and bit manipulation","title":"L1 datalab"},{"location":"CMU_15213/L1_datalab/#resources","text":"Bit Twiddling Hacks (stanford.edu) Lecture 16 \u2013 Signed Integers and Integer Arithmetic (umbc.edu)","title":"Resources"},{"location":"CMU_15213/L1_datalab/#takeaways","text":"Learn common bit ops and bit manipulation","title":"Takeaways"},{"location":"CMU_15213/L2_bomblab/","text":"Resources CS107 Guide to x86-64 CS107 GDB and Debugging Nagle\u5f20\u6728\u5320\u7684\u4e2a\u4eba\u7a7a\u95f4_\u54d4\u54e9\u54d4\u54e9_bilibili Mark Higgins \u2013 Medium Q & A: c - Display value found at given address gdb - Stack Overflow c++ - What's the difference between nexti and stepi in gdb? - Stack Overflow x86 Assembly/GNU assembly syntax - Wikibooks, open books for an open world bit manipulation - The difference between logical shift right, arithmetic shift right, and rotate right - Stack Overflow Takeaways GDB gdb <program> r info bre info reg ni # stay in the main si # step into the function break <function/address> x/<x/s/d> # shows you the contents of a memory address p/<format> # shows you the value stored in a named variable list cont Introduction - The Official Radare2 Book r2 <program> aaa pdf @ main~phase s sym.phase_5 pdf v # into visual mode c # toggle the cursor on/off Phase 1 Similar to activity 3 in recitation03-bomblab break phase_1 disas Dump of assembler code for function phase_1: => 0x0000000000400ee0 <+0>: sub $0x8,%rsp 0x0000000000400ee4 <+4>: mov $0x402400,%esi 0x0000000000400ee9 <+9>: callq 0x401338 <strings_not_equal> 0x0000000000400eee <+14>: test %eax,%eax 0x0000000000400ef0 <+16>: je 0x400ef7 <phase_1+23> 0x0000000000400ef2 <+18>: callq 0x40143a <explode_bomb> 0x0000000000400ef7 <+23>: add $0x8,%rsp 0x0000000000400efb <+27>: retq End of assembler dump. After disassembling, we can see that for callq strings_not_equal , it is using %esi as its argument. (gdb) x/s 0x402400 0x402400: \"Border relations with Canada have never been better.\" Phase 2 It starts from <phase_2+18> and loops between <phase_2+27> and <phase_2+52> . The key is <+14>: cmpl $0x1,(%rsp) and <+30>: add %eax,%eax . Phase 3 0x0000000000400f51 <+14>: mov $0x4025cf,%esi (gdb) x/s 0x4025cf 0x4025cf: \"%d %d\" The input format is two decimal numbers separated by a space. <+9>: lea 0x8(%rsp),%rdx` <+39>: cmpl $0x7,0x8(%rsp) <+44>: ja 0x400fad <phase_3+106> 4 bytes of the first number is compared to 0x7, pay attention to operand order for cmp , which is different in linux and windows. (gdb) x/2wd $rsp+0x8 0x7fffffffe548: 7 321 exam test inputs: 7 321 <+50>: jmpq *0x402470(,%rax,8) (gdb) x 0x402470 + 32 0x402490: 0x00400f91 go to dereferenced content in address 0x00400f91 0x0000000000400f91 <+78>: mov $0x185,%eax 0x0000000000400f96 <+83>: jmp 0x400fbe <phase_3+123> 0x0000000000400fbe <+123>: cmp 0xc(%rsp),%eax 0x0000000000400fc2 <+127>: je 0x400fc9 <phase_3+134> (gdb) p $rsp $72 = (void *) 0x7fffffffe540 (gdb) x/d 0x7fffffffe540+0xc 0x7fffffffe54c: 123 (gdb) p/d 0x185 $73 = 389 Phase 4 Look over phase_4 function and we can find: 0x000000000040101a <+14>: mov $0x4025cf,%esi (gdb) x/s 0x4025cf 0x4025cf: \"%d %d\" 0x0000000000401051 <+69>: cmpl $0x0,0xc(%rsp) 0x0000000000401056 <+74>: je 0x40105d <phase_4+81> 0x0000000000401058 <+76>: callq 0x40143a <explode_bomb> (gdb) x/wd $rsp+0x8 0x7fffffffe548: 10 (gdb) x/wd $rsp+0xc 0x7fffffffe54c: 123 second number should be 0 , here I tested with 10 123 . 0x000000000040102e <+34>: cmpl $0xe,0x8(%rsp) 0x0000000000401033 <+39>: jbe 0x40103a <phase_4+46> (gdb) x/d $rsp+0x8 0x7fffffffe548: 10 the first number should be less than or equal to 0xe=14 0x0000000000401048 <+60>: callq 0x400fce <func4> 0x000000000040104d <+65>: test %eax,%eax 0x000000000040104f <+67>: jne 0x401058 <phase_4+76> 0x0000000000401051 <+69>: cmpl $0x0,0xc(%rsp) 0x0000000000401056 <+74>: je 0x40105d <phase_4+81> 0x0000000000401058 <+76>: callq 0x40143a <explode_bomb> the second number should be 0 func4 is called and the result is compared to 0x0, if it is not 0x0, it will call explode_bomb . Then we disassemble func4 0x0000000000400fe2 <+20>: cmp %edi,%ecx 0x0000000000400fe4 <+22>: jle 0x400ff2 <func4+36> 0x0000000000400ff2 <+36>: mov $0x0,%eax 0x0000000000400ff7 <+41>: cmp %edi,%ecx 0x0000000000400ff9 <+43>: jge 0x401007 <func4+57> to let func4 return 0x0, the code at <+36>, we can find that the input x must be 7 <= x <= 7 . So the first argument is 7. Phase 5 CMU Bomb Lab with Radare2 \u2014 Phase 5 | by Mark Higgins | Medium I recommend using Radare2 , although it has a steep learning curve - Migration from ida, GDB or WinDBG - The Official Radare2 Book Phase 6 Really tricky that idk if it's worth the effort to defuse it. <+93> all numbers should be unique <+121> for each number x, we replace it with 7-x <+183> copy values into a new stack based on input (six numbers) <+257> values in the stack (linked list) should be sorted The key is to understand the linked list structure: 0x0000000000401183 <+143>: mov $0x6032d0,%edx (gdb) x/24wd 0x6032d0 0x6032d0 <node1>: 332 1 6304480 0 0x6032e0 <node2>: 168 2 6304496 0 0x6032f0 <node3>: 924 3 6304512 0 0x603300 <node4>: 691 4 6304528 0 0x603310 <node5>: 477 5 6304544 0 0x603320 <node6>: 443 6 0 0 Then all we need to do is to sort the values in the nodes, in this case, 332 168 924 691 477 443 to 924 691 477 443 332 168 . So, the node order is 3 4 5 6 1 2 and we replace each number x with 7-x: 4 3 2 1 6 5 .","title":"L2 bomblab"},{"location":"CMU_15213/L2_bomblab/#resources","text":"CS107 Guide to x86-64 CS107 GDB and Debugging Nagle\u5f20\u6728\u5320\u7684\u4e2a\u4eba\u7a7a\u95f4_\u54d4\u54e9\u54d4\u54e9_bilibili Mark Higgins \u2013 Medium Q & A: c - Display value found at given address gdb - Stack Overflow c++ - What's the difference between nexti and stepi in gdb? - Stack Overflow x86 Assembly/GNU assembly syntax - Wikibooks, open books for an open world bit manipulation - The difference between logical shift right, arithmetic shift right, and rotate right - Stack Overflow","title":"Resources"},{"location":"CMU_15213/L2_bomblab/#takeaways","text":"GDB gdb <program> r info bre info reg ni # stay in the main si # step into the function break <function/address> x/<x/s/d> # shows you the contents of a memory address p/<format> # shows you the value stored in a named variable list cont Introduction - The Official Radare2 Book r2 <program> aaa pdf @ main~phase s sym.phase_5 pdf v # into visual mode c # toggle the cursor on/off","title":"Takeaways"},{"location":"CMU_15213/L2_bomblab/#phase-1","text":"Similar to activity 3 in recitation03-bomblab break phase_1 disas Dump of assembler code for function phase_1: => 0x0000000000400ee0 <+0>: sub $0x8,%rsp 0x0000000000400ee4 <+4>: mov $0x402400,%esi 0x0000000000400ee9 <+9>: callq 0x401338 <strings_not_equal> 0x0000000000400eee <+14>: test %eax,%eax 0x0000000000400ef0 <+16>: je 0x400ef7 <phase_1+23> 0x0000000000400ef2 <+18>: callq 0x40143a <explode_bomb> 0x0000000000400ef7 <+23>: add $0x8,%rsp 0x0000000000400efb <+27>: retq End of assembler dump. After disassembling, we can see that for callq strings_not_equal , it is using %esi as its argument. (gdb) x/s 0x402400 0x402400: \"Border relations with Canada have never been better.\"","title":"Phase 1"},{"location":"CMU_15213/L2_bomblab/#phase-2","text":"It starts from <phase_2+18> and loops between <phase_2+27> and <phase_2+52> . The key is <+14>: cmpl $0x1,(%rsp) and <+30>: add %eax,%eax .","title":"Phase 2"},{"location":"CMU_15213/L2_bomblab/#phase-3","text":"0x0000000000400f51 <+14>: mov $0x4025cf,%esi (gdb) x/s 0x4025cf 0x4025cf: \"%d %d\" The input format is two decimal numbers separated by a space. <+9>: lea 0x8(%rsp),%rdx` <+39>: cmpl $0x7,0x8(%rsp) <+44>: ja 0x400fad <phase_3+106> 4 bytes of the first number is compared to 0x7, pay attention to operand order for cmp , which is different in linux and windows. (gdb) x/2wd $rsp+0x8 0x7fffffffe548: 7 321 exam test inputs: 7 321 <+50>: jmpq *0x402470(,%rax,8) (gdb) x 0x402470 + 32 0x402490: 0x00400f91 go to dereferenced content in address 0x00400f91 0x0000000000400f91 <+78>: mov $0x185,%eax 0x0000000000400f96 <+83>: jmp 0x400fbe <phase_3+123> 0x0000000000400fbe <+123>: cmp 0xc(%rsp),%eax 0x0000000000400fc2 <+127>: je 0x400fc9 <phase_3+134> (gdb) p $rsp $72 = (void *) 0x7fffffffe540 (gdb) x/d 0x7fffffffe540+0xc 0x7fffffffe54c: 123 (gdb) p/d 0x185 $73 = 389","title":"Phase 3"},{"location":"CMU_15213/L2_bomblab/#phase-4","text":"Look over phase_4 function and we can find: 0x000000000040101a <+14>: mov $0x4025cf,%esi (gdb) x/s 0x4025cf 0x4025cf: \"%d %d\" 0x0000000000401051 <+69>: cmpl $0x0,0xc(%rsp) 0x0000000000401056 <+74>: je 0x40105d <phase_4+81> 0x0000000000401058 <+76>: callq 0x40143a <explode_bomb> (gdb) x/wd $rsp+0x8 0x7fffffffe548: 10 (gdb) x/wd $rsp+0xc 0x7fffffffe54c: 123 second number should be 0 , here I tested with 10 123 . 0x000000000040102e <+34>: cmpl $0xe,0x8(%rsp) 0x0000000000401033 <+39>: jbe 0x40103a <phase_4+46> (gdb) x/d $rsp+0x8 0x7fffffffe548: 10 the first number should be less than or equal to 0xe=14 0x0000000000401048 <+60>: callq 0x400fce <func4> 0x000000000040104d <+65>: test %eax,%eax 0x000000000040104f <+67>: jne 0x401058 <phase_4+76> 0x0000000000401051 <+69>: cmpl $0x0,0xc(%rsp) 0x0000000000401056 <+74>: je 0x40105d <phase_4+81> 0x0000000000401058 <+76>: callq 0x40143a <explode_bomb> the second number should be 0 func4 is called and the result is compared to 0x0, if it is not 0x0, it will call explode_bomb . Then we disassemble func4 0x0000000000400fe2 <+20>: cmp %edi,%ecx 0x0000000000400fe4 <+22>: jle 0x400ff2 <func4+36> 0x0000000000400ff2 <+36>: mov $0x0,%eax 0x0000000000400ff7 <+41>: cmp %edi,%ecx 0x0000000000400ff9 <+43>: jge 0x401007 <func4+57> to let func4 return 0x0, the code at <+36>, we can find that the input x must be 7 <= x <= 7 . So the first argument is 7.","title":"Phase 4"},{"location":"CMU_15213/L2_bomblab/#phase-5","text":"CMU Bomb Lab with Radare2 \u2014 Phase 5 | by Mark Higgins | Medium I recommend using Radare2 , although it has a steep learning curve - Migration from ida, GDB or WinDBG - The Official Radare2 Book","title":"Phase 5"},{"location":"CMU_15213/L2_bomblab/#phase-6","text":"Really tricky that idk if it's worth the effort to defuse it. <+93> all numbers should be unique <+121> for each number x, we replace it with 7-x <+183> copy values into a new stack based on input (six numbers) <+257> values in the stack (linked list) should be sorted The key is to understand the linked list structure: 0x0000000000401183 <+143>: mov $0x6032d0,%edx (gdb) x/24wd 0x6032d0 0x6032d0 <node1>: 332 1 6304480 0 0x6032e0 <node2>: 168 2 6304496 0 0x6032f0 <node3>: 924 3 6304512 0 0x603300 <node4>: 691 4 6304528 0 0x603310 <node5>: 477 5 6304544 0 0x603320 <node6>: 443 6 0 0 Then all we need to do is to sort the values in the nodes, in this case, 332 168 924 691 477 443 to 924 691 477 443 332 168 . So, the node order is 3 4 5 6 1 2 and we replace each number x with 7-x: 4 3 2 1 6 5 .","title":"Phase 6"},{"location":"MIT_6.NULL/","text":"The Missing Semester of Your CS Education \u00b7 the missing semester of your cs education (mit.edu) Most of manuals of software used in the lecture can be found here: Software - GNU Project - Free Software Foundation e.g. Automatic Variables (GNU make) Resources \u8ba1\u7b97\u673a\u6559\u80b2\u4e2d\u7f3a\u5931\u7684\u4e00\u8bfe \u00b7 the missing semester of your cs education (missing-semester-cn.github.io) 2019 Lectures \u00b7 the missing semester of your cs education (mit.edu) Ivan-Kim/MIT-missing-semester: Complete solutions for 2020 MIT Missing Semester course for file in *; do mv \"$file\" `echo $file | tr ' ' '_'` ; done for file in [0-9]; do mv \"$file\" \"exercise_$file\"; done; for file in exercise_*; do mv \"$file\" \"${file#exercise_}\"; done;","title":"MIT 6.NULL"},{"location":"MIT_6.NULL/#resources","text":"\u8ba1\u7b97\u673a\u6559\u80b2\u4e2d\u7f3a\u5931\u7684\u4e00\u8bfe \u00b7 the missing semester of your cs education (missing-semester-cn.github.io) 2019 Lectures \u00b7 the missing semester of your cs education (mit.edu) Ivan-Kim/MIT-missing-semester: Complete solutions for 2020 MIT Missing Semester course for file in *; do mv \"$file\" `echo $file | tr ' ' '_'` ; done for file in [0-9]; do mv \"$file\" \"exercise_$file\"; done; for file in exercise_*; do mv \"$file\" \"${file#exercise_}\"; done;","title":"Resources"},{"location":"MIT_6.NULL/01_Course_overview_%2B_the_shell/","text":"Takeaways ll mkdir missing cd missing/ touch semester echo '#!/bin/sh' | tee semester echo 'curl --head --silent https://missing.csail.mit.edu' | tee -a semester chmod +x semester ./semester touch last-modified.txt | date -r semester > last-modified.txt cat last-modified.txt cat /sys/class/thermal/thermal_zone*/temp history | tail -n 20","title":"01 Course overview + the shell"},{"location":"MIT_6.NULL/01_Course_overview_%2B_the_shell/#takeaways","text":"ll mkdir missing cd missing/ touch semester echo '#!/bin/sh' | tee semester echo 'curl --head --silent https://missing.csail.mit.edu' | tee -a semester chmod +x semester ./semester touch last-modified.txt | date -r semester > last-modified.txt cat last-modified.txt cat /sys/class/thermal/thermal_zone*/temp history | tail -n 20","title":"Takeaways"},{"location":"MIT_6.NULL/02_Shell_Tools_and_Scripting/","text":"Takeaways Lear bash: Top (Bash Reference Manual) (gnu.org) especially: Bash Conditional Expressions (Bash Reference Manual) (gnu.org) The Set Builtin (Bash Reference Manual) (gnu.org) Parameter expansion [Bash Hackers Wiki] (bash-hackers.org) Manipulating Strings (tldp.org) Learn Bash Practical Bash - All you need to know to be comfortable in the terminal! - YouTube Bash Guide for Beginners (tldp.org) Advanced Bash-Scripting Guide (tldp.org) Manipulating Strings (tldp.org)","title":"02 Shell Tools and Scripting"},{"location":"MIT_6.NULL/02_Shell_Tools_and_Scripting/#takeaways","text":"Lear bash: Top (Bash Reference Manual) (gnu.org) especially: Bash Conditional Expressions (Bash Reference Manual) (gnu.org) The Set Builtin (Bash Reference Manual) (gnu.org) Parameter expansion [Bash Hackers Wiki] (bash-hackers.org) Manipulating Strings (tldp.org)","title":"Takeaways"},{"location":"MIT_6.NULL/02_Shell_Tools_and_Scripting/#learn-bash","text":"Practical Bash - All you need to know to be comfortable in the terminal! - YouTube Bash Guide for Beginners (tldp.org) Advanced Bash-Scripting Guide (tldp.org) Manipulating Strings (tldp.org)","title":"Learn Bash"},{"location":"MIT_6.NULL/04_Data_Wrangling/","text":"Takeaways cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq -c | sort -n | tail -n3 | awk '{print $2}' cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq | wc -l # missing combinations cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq > com_seen source com_all.sh > com_all comm -3 com_all com_seen > com_missing","title":"04 Data Wrangling"},{"location":"MIT_6.NULL/04_Data_Wrangling/#takeaways","text":"cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq -c | sort -n | tail -n3 | awk '{print $2}' cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq | wc -l # missing combinations cat /usr/share/dict/words | grep \".*a.*a.*a\" | grep -v \"'s$\" | sed -E \"s/.*(..)/\\1/\" | sort | uniq > com_seen source com_all.sh > com_all comm -3 com_all com_seen > com_missing","title":"Takeaways"},{"location":"MIT_6.NULL/05_Command-line_Environment/","text":"Takeaways # job control $ sleep 1000 ^Z [1] + 18653 suspended sleep 1000 $ nohup sleep 2000 & [2] 18745 appending output to nohup.out $ jobs [1] + suspended sleep 1000 [2] - running nohup sleep 2000 $ bg %1 [1] - 18653 continued sleep 1000 $ jobs [1] - running sleep 1000 [2] + running nohup sleep 2000 $ kill -STOP %1 [1] + 18653 suspended (signal) sleep 1000 $ jobs [1] + suspended (signal) sleep 1000 [2] - running nohup sleep 2000 $ kill -SIGHUP %1 [1] + 18653 hangup sleep 1000 $ jobs [2] + running nohup sleep 2000 $ kill -SIGHUP %2 $ jobs [2] + running nohup sleep 2000 $ kill %2 [2] + 18745 terminated nohup sleep 2000 $ jobs # ssh ssh-keygen -t ed25519 -C \"your_email@example.com\" eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519 # add the public key to remote cat ~/.ssh/id_ed25519.pub | ssh <user>@<remote> 'cat >> ~/.ssh/authorized_keys' # OR ssh-copy-id -i ~/.ssh/id_ed25519.pub <user>@<remote> ssh port forwarding Tmux Cheat Sheet & Quick Reference # ~/.ssh/config Host vm User foobar HostName 172.16.174.141 Port 2222 IdentityFile ~/.ssh/id_ed25519 LocalForward 9999 localhost:8888 # Configs can also take wildcards Host *.mit.edu User foobaz # In most cases, after copying the public key to remote_server/authorized_keys # Only thing needed to configure is the following settings in config file Host vs HostName <ip address: 0.0.0.0> User <username: lqi> Exercises sleep 60 & pgrep -f \"sleep 60\" | wait && ls sleep 600 & # open a new session # jobs: Display status of jobs in the current session. jobs -l # show nothing because it's in a new session # ps aux: Display status of all sessions # start a new shell session and run the following command ps aux | pgrep -f \"sleep 600\" . pidwait.sh $(ps aux | pgrep -f \"sleep 600\")","title":"05 Command line Environment"},{"location":"MIT_6.NULL/05_Command-line_Environment/#takeaways","text":"# job control $ sleep 1000 ^Z [1] + 18653 suspended sleep 1000 $ nohup sleep 2000 & [2] 18745 appending output to nohup.out $ jobs [1] + suspended sleep 1000 [2] - running nohup sleep 2000 $ bg %1 [1] - 18653 continued sleep 1000 $ jobs [1] - running sleep 1000 [2] + running nohup sleep 2000 $ kill -STOP %1 [1] + 18653 suspended (signal) sleep 1000 $ jobs [1] + suspended (signal) sleep 1000 [2] - running nohup sleep 2000 $ kill -SIGHUP %1 [1] + 18653 hangup sleep 1000 $ jobs [2] + running nohup sleep 2000 $ kill -SIGHUP %2 $ jobs [2] + running nohup sleep 2000 $ kill %2 [2] + 18745 terminated nohup sleep 2000 $ jobs # ssh ssh-keygen -t ed25519 -C \"your_email@example.com\" eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519 # add the public key to remote cat ~/.ssh/id_ed25519.pub | ssh <user>@<remote> 'cat >> ~/.ssh/authorized_keys' # OR ssh-copy-id -i ~/.ssh/id_ed25519.pub <user>@<remote> ssh port forwarding Tmux Cheat Sheet & Quick Reference # ~/.ssh/config Host vm User foobar HostName 172.16.174.141 Port 2222 IdentityFile ~/.ssh/id_ed25519 LocalForward 9999 localhost:8888 # Configs can also take wildcards Host *.mit.edu User foobaz # In most cases, after copying the public key to remote_server/authorized_keys # Only thing needed to configure is the following settings in config file Host vs HostName <ip address: 0.0.0.0> User <username: lqi>","title":"Takeaways"},{"location":"MIT_6.NULL/05_Command-line_Environment/#exercises","text":"sleep 60 & pgrep -f \"sleep 60\" | wait && ls sleep 600 & # open a new session # jobs: Display status of jobs in the current session. jobs -l # show nothing because it's in a new session # ps aux: Display status of all sessions # start a new shell session and run the following command ps aux | pgrep -f \"sleep 600\" . pidwait.sh $(ps aux | pgrep -f \"sleep 600\")","title":"Exercises"},{"location":"MIT_6.NULL/06_Version_Control_%28Git%29/","text":"Takeaways Git 101 Exercises # zsh using git plugin # play around with all git aliases # e.g. alias | grep 'git log' # visualize log glog # check history of certain file glg README.md # glgg # glgga # commit associated with a certain content in a file gbl README.md # search for the content e.g. `jobs` like in the vi/vim editor /jobs # get the hash for the commit e.g. f0931819 gsh f0931819 # remove large/sensitive data git filter-branch --force --index-filter \\ \"git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\" \\ --prune-empty --tag-name-filter cat -- --all # git stash performs like a stack # it pushes changes (no untracked files by default otherwise add -u including untracked files) to a stack (dirty working dir) # and git stash pop (gstp) to turn changes back","title":"06 Version Control (Git)"},{"location":"MIT_6.NULL/06_Version_Control_%28Git%29/#takeaways","text":"Git 101","title":"Takeaways"},{"location":"MIT_6.NULL/06_Version_Control_%28Git%29/#exercises","text":"# zsh using git plugin # play around with all git aliases # e.g. alias | grep 'git log' # visualize log glog # check history of certain file glg README.md # glgg # glgga # commit associated with a certain content in a file gbl README.md # search for the content e.g. `jobs` like in the vi/vim editor /jobs # get the hash for the commit e.g. f0931819 gsh f0931819 # remove large/sensitive data git filter-branch --force --index-filter \\ \"git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\" \\ --prune-empty --tag-name-filter cat -- --all # git stash performs like a stack # it pushes changes (no untracked files by default otherwise add -u including untracked files) to a stack (dirty working dir) # and git stash pop (gstp) to turn changes back","title":"Exercises"},{"location":"MIT_6.NULL/07_Debugging_and_Profiling/","text":"Takeaways online shell scripting checker ShellCheck \u2013 shell script analysis tool hyperfine for blackbox-benmarking sharkdp/hyperfine: A command-line benchmarking tool (github.com) call graphs for code/file structrue Command-line Usage \u2014 Python Call Graph 1.0.1 documentation (slowchop.com) profile a Python script # check corresponding manpage for detailed usage e.g. sort by time etc. python -m cProfile script.py python -m line_profiler script.py python -m memory_profiler script.py","title":"07 Debugging and Profiling"},{"location":"MIT_6.NULL/07_Debugging_and_Profiling/#takeaways","text":"online shell scripting checker ShellCheck \u2013 shell script analysis tool hyperfine for blackbox-benmarking sharkdp/hyperfine: A command-line benchmarking tool (github.com) call graphs for code/file structrue Command-line Usage \u2014 Python Call Graph 1.0.1 documentation (slowchop.com) profile a Python script # check corresponding manpage for detailed usage e.g. sort by time etc. python -m cProfile script.py python -m line_profiler script.py python -m memory_profiler script.py","title":"Takeaways"},{"location":"MIT_6.NULL/08_Metaprogramming/","text":"Version controls for dependencies Take a look at the various ways to specify version requirements for dependencies in Rust\u2019s build system . Most package repositories support similar syntax. For each one (caret, tilde, wildcard, comparison, and multiple), try to come up with a use-case in which that particular kind of requirement makes sense. major.minor.patch Release increments number of: patch : API hasn\u2019t changed minor : API has new, backwards-compatible changes major : API has backwards-incompatible change Specifying Dependencies - The Cargo Book (rust-lang.org) # Caret requirements # An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. ^1.2.3 := >=1.2.3, <2.0.0 ^1.2 := >=1.2.0, <2.0.0 ^1 := >=1.0.0, <2.0.0 ^0.2.3 := >=0.2.3, <0.3.0 ^0.2 := >=0.2.0, <0.3.0 ^0.0.3 := >=0.0.3, <0.0.4 ^0.0 := >=0.0.0, <0.1.0 ^0 := >=0.0.0, <1.0.0 # Tilde requirements # If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed. ~1.2.3 := >=1.2.3, <1.3.0 ~1.2 := >=1.2.0, <1.3.0 ~1 := >=1.0.0, <2.0.0 # Wildcard requirements * := >=0.0.0 1.* := >=1.0.0, <2.0.0 1.2.* := >=1.2.0, <1.3.0 Make: Top (GNU make) especially: Automatic Variables (GNU make) TLDR: Makefile Tutorial By Example There's a nice list of options that can be run from make. Check out --dry-run , --touch , --old-file . You can have multiple targets to make, i.e. make clean run test runs the clean goal, then run , and then test . A Simple Makefile Tutorial (colby.edu) Note that order of entries in Makefile matters - it begins the first rule by default, then the rest is dependencies oriented. That said, if some latter targets are not dependencies of previous rules, they might not be executed Some mistake I made: <mark> c - How do I make Makefile to recompile only changed files? - Stack Overflow </mark> In short, makefile treat target name as file name. If no target files exist: it will recompile it. If target files exist: it won't recompile it. paper.pdf: paper.tex plot-data.png pdflatex paper.tex plot-%.png: %.dat plot.py ./plot.py -i $*.dat -o $@ .PHONY: clean # clean is a phony target that is not a file named clean. clean: git ls-files -o | xargs rm -f similar command from pre-commit git ls-files -- '*.py' | xargs pre-commit run --files : run all hooks against all *.py files in the repository. .git/hooks/pre-commit #!/bin/sh # https://jsinkers.github.io/notes/notebooks/missing_class/08_metaprogramming.html # Pre-commit script to prevent commit if the make fails # Redirect output to stderr. exec 1>&2 if make then echo \"Make successful\" else cat << EOF Error: could not make pdf EOF exit 1 fi # OR run spefic command only, in this case paper.pdf entry only #!/bin/sh if ! make paper.pdf ; then echo \"Cannot make paper.pdf\" exit 1 fi Github workflows - GitHub Pages Compare common stacks that are known to be good for rendering static websites. Mkdocs Mkdocs has some issues: Navigation not in alphanumeric order (when pages config is automatic) \u00b7 Issue #638 \u00b7 mkdocs/mkdocs (github.com) that can be solved through: lukasgeiter/mkdocs-awesome-pages-plugin: An MkDocs plugin that simplifies configuring page titles and their order courses/update_navigation_order.sh Anyways, it orders pages based on lexicographical order by default. This saves a lot of time compared to manually generating or writing specific indexing file in order to maintain the navigation bar. Jekyll Jekyll provides the most beautiful UI/themes from what I can tell - but its navigation bar needs to be manually maintained/hard-coded. To get something in quickly: stanford-cs329s/reports: Final reports for CS 329S Winter 2021 Based on Jekyll, I love Just the Docs , which also requires hard-coding the navigation bar, but it's a lot ?convenient using in-page YAML front matter Navigation Structure | Just the Docs rnnh/bioinfo-notebook: \ud83d\udd2c Bioinformatics Notebook. Scripts for bioinformatics pipelines, with quick start guides for programs and video demonstrations. Docsify Docsify is installed through npm which is not installed in the server that I am currently using - don't want to install it on a industry server without sudo Also, it need to hard-code navigation bar like jekyll using docsify/_sidebar.md at master \u00b7 docsifyjs/docsify Read the Docs It needs to hard-code navigation bar using index.rst : marcelm/cutadapt: Cutadapt removes adapter sequences from sequencing reads Like Sphinx Tutorial \u2014 Sphinx Tutorial 1.0 documentation , the main advantage is that it provides version controls - really easy to check old versions of the docs. Overall, I personally prefer to just-the-docs/just-the-docs: A modern, high customizable, responsive Jekyll theme for documention with built-in search. , but I am not sure if it is the best choice.","title":"08 Metaprogramming"},{"location":"MIT_6.NULL/08_Metaprogramming/#version-controls-for-dependencies","text":"Take a look at the various ways to specify version requirements for dependencies in Rust\u2019s build system . Most package repositories support similar syntax. For each one (caret, tilde, wildcard, comparison, and multiple), try to come up with a use-case in which that particular kind of requirement makes sense. major.minor.patch Release increments number of: patch : API hasn\u2019t changed minor : API has new, backwards-compatible changes major : API has backwards-incompatible change Specifying Dependencies - The Cargo Book (rust-lang.org) # Caret requirements # An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. ^1.2.3 := >=1.2.3, <2.0.0 ^1.2 := >=1.2.0, <2.0.0 ^1 := >=1.0.0, <2.0.0 ^0.2.3 := >=0.2.3, <0.3.0 ^0.2 := >=0.2.0, <0.3.0 ^0.0.3 := >=0.0.3, <0.0.4 ^0.0 := >=0.0.0, <0.1.0 ^0 := >=0.0.0, <1.0.0 # Tilde requirements # If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed. ~1.2.3 := >=1.2.3, <1.3.0 ~1.2 := >=1.2.0, <1.3.0 ~1 := >=1.0.0, <2.0.0 # Wildcard requirements * := >=0.0.0 1.* := >=1.0.0, <2.0.0 1.2.* := >=1.2.0, <1.3.0","title":"Version controls for dependencies"},{"location":"MIT_6.NULL/08_Metaprogramming/#make-top-gnu-make","text":"especially: Automatic Variables (GNU make) TLDR: Makefile Tutorial By Example There's a nice list of options that can be run from make. Check out --dry-run , --touch , --old-file . You can have multiple targets to make, i.e. make clean run test runs the clean goal, then run , and then test . A Simple Makefile Tutorial (colby.edu) Note that order of entries in Makefile matters - it begins the first rule by default, then the rest is dependencies oriented. That said, if some latter targets are not dependencies of previous rules, they might not be executed Some mistake I made: <mark> c - How do I make Makefile to recompile only changed files? - Stack Overflow </mark> In short, makefile treat target name as file name. If no target files exist: it will recompile it. If target files exist: it won't recompile it. paper.pdf: paper.tex plot-data.png pdflatex paper.tex plot-%.png: %.dat plot.py ./plot.py -i $*.dat -o $@ .PHONY: clean # clean is a phony target that is not a file named clean. clean: git ls-files -o | xargs rm -f similar command from pre-commit git ls-files -- '*.py' | xargs pre-commit run --files : run all hooks against all *.py files in the repository. .git/hooks/pre-commit #!/bin/sh # https://jsinkers.github.io/notes/notebooks/missing_class/08_metaprogramming.html # Pre-commit script to prevent commit if the make fails # Redirect output to stderr. exec 1>&2 if make then echo \"Make successful\" else cat << EOF Error: could not make pdf EOF exit 1 fi # OR run spefic command only, in this case paper.pdf entry only #!/bin/sh if ! make paper.pdf ; then echo \"Cannot make paper.pdf\" exit 1 fi","title":"Make:  Top (GNU make)"},{"location":"MIT_6.NULL/08_Metaprogramming/#github-workflows-github-pages","text":"Compare common stacks that are known to be good for rendering static websites.","title":"Github workflows - GitHub Pages"},{"location":"MIT_6.NULL/08_Metaprogramming/#mkdocs","text":"Mkdocs has some issues: Navigation not in alphanumeric order (when pages config is automatic) \u00b7 Issue #638 \u00b7 mkdocs/mkdocs (github.com) that can be solved through: lukasgeiter/mkdocs-awesome-pages-plugin: An MkDocs plugin that simplifies configuring page titles and their order courses/update_navigation_order.sh Anyways, it orders pages based on lexicographical order by default. This saves a lot of time compared to manually generating or writing specific indexing file in order to maintain the navigation bar.","title":"Mkdocs"},{"location":"MIT_6.NULL/08_Metaprogramming/#jekyll","text":"Jekyll provides the most beautiful UI/themes from what I can tell - but its navigation bar needs to be manually maintained/hard-coded. To get something in quickly: stanford-cs329s/reports: Final reports for CS 329S Winter 2021 Based on Jekyll, I love Just the Docs , which also requires hard-coding the navigation bar, but it's a lot ?convenient using in-page YAML front matter Navigation Structure | Just the Docs rnnh/bioinfo-notebook: \ud83d\udd2c Bioinformatics Notebook. Scripts for bioinformatics pipelines, with quick start guides for programs and video demonstrations.","title":"Jekyll"},{"location":"MIT_6.NULL/08_Metaprogramming/#docsify","text":"Docsify is installed through npm which is not installed in the server that I am currently using - don't want to install it on a industry server without sudo Also, it need to hard-code navigation bar like jekyll using docsify/_sidebar.md at master \u00b7 docsifyjs/docsify","title":"Docsify"},{"location":"MIT_6.NULL/08_Metaprogramming/#read-the-docs","text":"It needs to hard-code navigation bar using index.rst : marcelm/cutadapt: Cutadapt removes adapter sequences from sequencing reads Like Sphinx Tutorial \u2014 Sphinx Tutorial 1.0 documentation , the main advantage is that it provides version controls - really easy to check old versions of the docs. Overall, I personally prefer to just-the-docs/just-the-docs: A modern, high customizable, responsive Jekyll theme for documention with built-in search. , but I am not sure if it is the best choice.","title":"Read the Docs"},{"location":"MIT_6.NULL/09_Security_and_Cryptography/","text":"Exercises Entropy four concatenations: log2(10^20)\u224866.4 62 possibilities > log2(62^8)\u224847.6 previous one with larger entropy 3*10^8 years and 692 years bash % sha256sum debian-mac-11.2.0-amd64-netinst.iso | shasum --algorithm 256 --check debian-mac-11.2.0-amd64-netinst.iso: OK bash $ openssl aes-256-cbc -salt -in secret.txt -out secret.enc.txt enter aes-256-cbc encryption password: Verifying - enter aes-256-cbc encryption password: $ openssl aes-256-cbc -d -in secret.enc.txt -out secret.dec.txt enter aes-256-cbc decryption password: $ cmp secret.txt secret.dec.txt | echo $? 0","title":"09 Security and Cryptography"},{"location":"MIT_6.NULL/09_Security_and_Cryptography/#exercises","text":"Entropy four concatenations: log2(10^20)\u224866.4 62 possibilities > log2(62^8)\u224847.6 previous one with larger entropy 3*10^8 years and 692 years bash % sha256sum debian-mac-11.2.0-amd64-netinst.iso | shasum --algorithm 256 --check debian-mac-11.2.0-amd64-netinst.iso: OK bash $ openssl aes-256-cbc -salt -in secret.txt -out secret.enc.txt enter aes-256-cbc encryption password: Verifying - enter aes-256-cbc encryption password: $ openssl aes-256-cbc -d -in secret.enc.txt -out secret.dec.txt enter aes-256-cbc decryption password: $ cmp secret.txt secret.dec.txt | echo $? 0","title":"Exercises"},{"location":"MIT_6.NULL/10_Potpourri/","text":"TAKEAWAYS Really Potpourri and hard to conclude those topics, just read over the lecturer notes carefully: Potpourri \u00b7 the missing semester of your cs education (mit.edu) Some tools I personally like: libfuse/sshfs: A network filesystem client to connect to SSH servers (github.com) rclone mount WireGuard: fast, modern, secure VPN tunnel Hammerspoon - dotfiles-local/hammerspoon at mac \u00b7 anishathalye/dotfiles-local (github.com) unetbootin/unetbootin: UNetbootin installs Linux/BSD distributions to a partition or USB drive (github.com) ventoy/Ventoy: A new bootable USB solution. (github.com)","title":"10 Potpourri"},{"location":"MIT_6.NULL/10_Potpourri/#takeaways","text":"Really Potpourri and hard to conclude those topics, just read over the lecturer notes carefully: Potpourri \u00b7 the missing semester of your cs education (mit.edu) Some tools I personally like: libfuse/sshfs: A network filesystem client to connect to SSH servers (github.com) rclone mount WireGuard: fast, modern, secure VPN tunnel Hammerspoon - dotfiles-local/hammerspoon at mac \u00b7 anishathalye/dotfiles-local (github.com) unetbootin/unetbootin: UNetbootin installs Linux/BSD distributions to a partition or USB drive (github.com) ventoy/Ventoy: A new bootable USB solution. (github.com)","title":"TAKEAWAYS"},{"location":"Stanford_Machine_Learning_Coursera/","text":"Machine Learning | Coursera As I am already familiar with machine learning before taking this course, the main purpose here is to quickly brush up on some concepts - not for programming assignments. Practical Machine Learning \u2014 Practical Machine Learning (d2l.ai) \u8ddf\u674e\u6c90\u5b66AI\u7684\u4e2a\u4eba\u7a7a\u95f4_\u54d4\u54e9\u54d4\u54e9_bilibili","title":"Stanford Machine Learning Coursera"},{"location":"Stanford_Machine_Learning_Coursera/Week_1-5/","text":"Mainly focused on topics like linear models like linear regression / logistic regression and neural networks Gradient Descent Gradient Descent requires simultaneous update - updating all parameters at the same time using the same learning rate instead of updating one parameter first, and then use updated parameter to calculate another parameter. Logistic regression model logistic = feeding linear regression model \\(y = \\beta \\cdot x\\) into sigmoid function \\[ p(x) = \\frac{b^{\\beta \\cdot x}}{1+b^{\\beta \\cdot x}}= \\frac{1}{1+b^{-\\beta \\cdot x}}=S_b(t) \\] Cost functions for above model - It has been really hard for me to memorize the cost function for logistic regression. Hopefully, the picture below helps capture the intuition of how to design cost function. \\(h_{\\theta}x\\) means hypothesis of model regarding relationships between \\(\\theta\\) and \\(x\\) linear regression: \\(h_{\\theta}x = \\theta^Tx\\) logistic regression: \\(h_{\\theta}x = \\frac{1}{1+e^{-\\theta^Tx}}\\) Another post explaining loss function in logistic regression: Loss Function (Part II): Logistic Regression | by Shuyu Luo | Towards Data Science In multi-classification problem, one-vs-all provides a brute force way to solve the problem: Calculate manually in logistic regression Regularization Intuition in regularization: Cost Function | Coursera Note generally we don't know which \\(\\theta\\) to shrink i.e. which parameter OR parameters to shrink explicitly, but we want to remove the overall influence and penalize all parameters. That's why we use \\(\\lambda \\sum\\limits_{j=1}^{n} \\theta_{j}^2\\) in which \\(\\lambda\\) capture the intuition of the extend of penalization. The whole cost function consists of original cost function and an additional regularization term. So the larger \\(\\lambda\\) means the higher weight it has in the final cost function, meaning the regularization has higher amount of impact. Thus, in extreme case with very large regularization, the function can be underfitted. Likewise, the larger \\(C = 1/\\lambda\\) which is the parameter for original cost function (non regularization part) means less impact of the regularization. Thus, in extreme case with no regularization, the function is trying to do perfect job even it comes with overfitting. machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com) Neural Networks The main motivation behind neural network is that it can introduce non-linearity into the problems that can't be solved through linear models Examples and Intuitions I | Coursera Backpropagation - The following post, I believe, gives a better explanation: A Step by Step Backpropagation Example \u2013 Matt Mazur . The network usually starts with random initialization for all initial weights. Backpropagation provides the \"formal\" way to compute gradient using chain rule i.e. you know exactly where to go upfront to optimize cost function. In contrast, gradient descent is like testing all directions to find the best direction.","title":"Week 1 5"},{"location":"Stanford_Machine_Learning_Coursera/Week_1-5/#gradient-descent","text":"Gradient Descent requires simultaneous update - updating all parameters at the same time using the same learning rate instead of updating one parameter first, and then use updated parameter to calculate another parameter.","title":"Gradient Descent"},{"location":"Stanford_Machine_Learning_Coursera/Week_1-5/#logistic-regression-model","text":"logistic = feeding linear regression model \\(y = \\beta \\cdot x\\) into sigmoid function \\[ p(x) = \\frac{b^{\\beta \\cdot x}}{1+b^{\\beta \\cdot x}}= \\frac{1}{1+b^{-\\beta \\cdot x}}=S_b(t) \\] Cost functions for above model - It has been really hard for me to memorize the cost function for logistic regression. Hopefully, the picture below helps capture the intuition of how to design cost function. \\(h_{\\theta}x\\) means hypothesis of model regarding relationships between \\(\\theta\\) and \\(x\\) linear regression: \\(h_{\\theta}x = \\theta^Tx\\) logistic regression: \\(h_{\\theta}x = \\frac{1}{1+e^{-\\theta^Tx}}\\) Another post explaining loss function in logistic regression: Loss Function (Part II): Logistic Regression | by Shuyu Luo | Towards Data Science In multi-classification problem, one-vs-all provides a brute force way to solve the problem: Calculate manually in logistic regression","title":"Logistic regression model"},{"location":"Stanford_Machine_Learning_Coursera/Week_1-5/#regularization","text":"Intuition in regularization: Cost Function | Coursera Note generally we don't know which \\(\\theta\\) to shrink i.e. which parameter OR parameters to shrink explicitly, but we want to remove the overall influence and penalize all parameters. That's why we use \\(\\lambda \\sum\\limits_{j=1}^{n} \\theta_{j}^2\\) in which \\(\\lambda\\) capture the intuition of the extend of penalization. The whole cost function consists of original cost function and an additional regularization term. So the larger \\(\\lambda\\) means the higher weight it has in the final cost function, meaning the regularization has higher amount of impact. Thus, in extreme case with very large regularization, the function can be underfitted. Likewise, the larger \\(C = 1/\\lambda\\) which is the parameter for original cost function (non regularization part) means less impact of the regularization. Thus, in extreme case with no regularization, the function is trying to do perfect job even it comes with overfitting. machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com)","title":"Regularization"},{"location":"Stanford_Machine_Learning_Coursera/Week_1-5/#neural-networks","text":"The main motivation behind neural network is that it can introduce non-linearity into the problems that can't be solved through linear models Examples and Intuitions I | Coursera Backpropagation - The following post, I believe, gives a better explanation: A Step by Step Backpropagation Example \u2013 Matt Mazur . The network usually starts with random initialization for all initial weights. Backpropagation provides the \"formal\" way to compute gradient using chain rule i.e. you know exactly where to go upfront to optimize cost function. In contrast, gradient descent is like testing all directions to find the best direction.","title":"Neural Networks"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/","text":"Metrics in Machine Learning System Design Bias and variance: Diagnosing Bias vs. Variance | Coursera \\(J_{CV}\\) denotes cross validation error which is the same as validation error \\(J_V\\) Precision vs. Recall Note that, in convention, \\(y=1\\) usually presents rare class OR class we are more interested in e.g. patients with cancer we want to detect PR curve Note the PR curve gives precision and recall at all thresholds - high threshold gives high precision but low recall, low threshold gives low precision but high recall (this is what we want in the cancer example) How to choose a good threshold? A: use F-score, \\(F_1 Score = 2\\frac{PR}{P+R}\\) , in mathematics, this is called the harmonic mean of precision and sensitivity. SVM The intuition of designing loss function for logistic regression should gives some clue how we design it in SVM: I think I mentioned it previously: The whole cost function consists of original cost function and an additional regularization term. So the larger \\(\\lambda\\) means the higher weight it has in the final cost function, meaning the regularization has higher amount of impact. Thus, in extreme case with very large regularization, the function can be underfitted. Likewise, the larger \\(C = 1/\\lambda\\) which is the parameter for original cost function (non regularization part) means less impact of the regularization. Thus, in extreme case with no regularization, the function is trying to do perfect job even it comes with overfitting. machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com) So here, Andrew is trying to illustrate the case that SVM is trying to separate two classes without any errors: Udacity provides another understanding of the optimization problem. For two vectors \\(x_{1},x_{2}\\) that are on two support vectors we have: \\[ \\begin{array}{l}{\\omega^{\\top} x_{1}+b=1} \\\\ {\\omega^{\\top} x_{2}+b=-1}\\end{array} \\] If you subtract them, the distance between planes (i.e. margin) can be presented as \\[ \\frac{\\omega^{T}\\left(x_{1}-x_{2}\\right)}{\\|\\omega\\|}=\\frac{2}{\\|\\omega\\|} \\] s.t. for two classifications/labels \\(y_i = 1/-1\\) , \\(y_i*(w^Tx_i+b) \\geq 1\\) Why? Because \\(w^T\\) is the direction vertical to the hyperplane so the left part means exactly the projected distance of the vector \\(x_{1} - x_{2}\\) on unit vector \\(w^T / \\|w^T\\|\\) Why \\(w^T\\) is vertical to the hyperplane? Imagine two points on the hyperplane \\(w^Tx+b =0\\) \\(w^Tx_1+b = w^Tx_2+b = 0\\) \\(w^T(x_1-x_2)=0\\) \\(x_{1} - x_{2}\\) is the vector on the hyperplane and thus \\(w ^T\\) is the normal vector. bam!!! Maximizing the margin equals to minimizing the reciprocal along with monotone \\[ \\begin{array}{l}{\\max \\frac{2}{\\| w \\|}} \\\\ {\\min 1 / 2\\|w\\|^{2}}\\end{array} \\] Polynomial Kernel I personally prefer explanations from Josh here: Video Index - StatQuest!!! (there are three videos) Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3) - YouTube Polynomial kernel formula: \\[ (a * b + r)^d \\\\ a.b: \\text{a and b refer to two different data in the dataset (1 dimension)} \\\\ r: \\text{r determines the coefficient of polynomial} \\\\ d: \\text{d determines the degree of polynomial} \\] Note \\(r\\) and \\(d\\) are determined through cross validation, here we set \\(r = 1\\) and \\(d = 2\\) for illustration \\[ (a*b+1)^2 = (\\sqrt{2}a,a^2,1)\\cdot(\\sqrt{2}b,b^2,1) \\] It means we create two new features (constant 1 is the same so we ignore it): \\(\\sqrt{2}*x, x^2\\) to help differentiate original \\(a\\) and \\(b\\) where there is only one feature \\(x\\) In practice, we can directly plug values into the kernel where the output represents the relationship between two data in 2-dimensions without actually transform the data to 2-Dimensions. Radial Kernel Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3) - YouTube RBF formula: \\[ e^{-\\gamma(a - b)^2} \\\\ \\gamma: \\text{determines the scale of influence two points have on each other} \\] Go through the video and you will find <mark> how RBF kernel defines the relationship between two points in infinite-dimensions is genius! </mark> Let's set \\(\\gamma=1/2\\) for illustration: \\(e^{-1/2(a - b)^2} = e^{-1/2(a^2+b^2)} e^{ab}\\) With Taylor Series Expansion: \\(e^x = f(a)+\\frac {f'(a)}{1!} (x-a)+ \\frac{f''(a)}{2!} (x-a)^2+\\frac{f'''(a)}{3!}(x-a)^3+ \\cdots\\) We set \\(a = 0\\) for series above and replace x with ab So, for original coordinates that has only one dimension \\(x\\) , the new coordinates are of infinite number of dimensions. Note, like mentioned before, we are not actually projecting data into infinite number of dimensions and then try to figure out how to separate them in SVM. Instead, we plug the value into kernel function where the output (a single number) represents the relationship between two data points. K-Means from statistics import mean def cal_dist(p1,p2): return sum((a-b)**2 for a,b in zip(p1,p2))**0.5 def kmeans(dataset,k,threshold=1e-10,n_iter=50): \"\"\" Args: dataset : array k : number of K threshold : centroids merge threshold e.g. dist(prev_centroid,new_centroid) <= thrshold == same_centroid n_iter : maximum number of iterations Returns: location of final centroids \"\"\" dataset = [tuple(i) for i in dataset] prev_centroids = {i: [] for i in dataset[:k]} for n in range(1, n_iter + 1): for data in dataset: # step1: assign datapoint to its closest centroid closest_centroid = min(prev_centroids,key=lambda x:cal_dist(data,x)) prev_centroids[closest_centroid].append(data) cur_centroids = [] for centroid, cluster in prev_centroids.items(): # calculate new centroid mean_x,mean_y = mean(c[0] for c in cluster),mean(c[1] for c in cluster) cur_centroids.append((mean_x,mean_y)) # return if all new centroids are close (dist <= threshold) to old centroids if all(cal_dist(p,q) <= threshold for p,q in zip(prev_centroids.keys(),cur_centroids)): print(f'Optimization finished with {n} iterations') for k,v in prev_centroids.items(): print(k,':',v) break # update prev_centroids with cur_centroids prev_centroids = {centroid:[] for centroid in cur_centroids} return cur_centroids # A = [(random.randint(0,10),random.randint(0,10)) for _ in range(1000)] A = [[1,1],[2,1],[4,3],[5,4],[100,10],[20,20]] print(kmeans(A,2)) Elbow Method When trying to determine number of K in K-Means, we can use \"elbow\" method. Similarly in PCA, when trying to determine the number of principal component for further analysis, we can use this method in scree plot. From my previous manuscript: However, inferentially determining the number of PCs remains a difficult task and there is no single approach. To address this problem, 10-fold cross-validation was used. The first 50 PCs were used in cross-validation while always keeping other covariates. The overall root-mean-square error (RMSE) and R-squared would be our metrics when evaluating model performance. Note that the smallest RMSE or the largest R-squared doesn\u2019t significantly indicate the best number of PCs to use in the model. Overfitting may occur and affect the predictive accuracy of the regression model with a new dataset. As such, a nested model search was performed using F-tests on groupings of variables. Thus, we compared nested models without any principal components of daily activity and models with different numbers of components. GraphPad Prism 9 Curve Fitting Guide - How the F test works to compare models PCA The mathematics to inferentially understand why calculating eigen decomposition of covariance matrix == getting the principal component that minimize the distance to PC/maximize the variance of projections is included in my github repo. eigen-decomposition of covariance matrix import numpy as np def pca(data): # scale data based on mean of features feature_mean = np.mean(data.T,axis=1) data_std = data - feature_mean covMatrix = np.cov(data_std.T) eigenValues, eigenVectors = np.linalg.eig(covMatrix) # eigenVectors: The normalized (unit \u201clength\u201d) eigenvectors, such that the column v[:,i] is the eigenvector corresponding to the eigenvalue w[i]. idx = eigenValues.argsort()[::-1] eigenValues = eigenValues[idx] eigenVectors = eigenVectors[:,idx] projections = data_std @ eigenVectors # definitions of dot product of two vectors/matrices print(eigenValues) print(eigenVectors) print(projections) # projections[0] is the projection of data_std[0] in the new coordinates x = [-1,-1,0,2,0] y = [-2,0,0,1,1] data = np.array([[i,j] for i,j in zip(x,y)]) pca(data) # [2.5 0.5] # [[ 0.70710678 -0.70710678] # [ 0.70710678 0.70710678]] # [[-2.12132034 -0.70710678] # [-0.70710678 0.70710678] # [ 0. 0. ] # [ 2.12132034 -0.70710678] # [ 0.70710678 0.70710678]] SVD The SVD decomposition is heavily linear algebra required, please read through some main concepts here: Symmetric matrix - Wikipedia Eigendecomposition of a matrix - Wikipedia dimensionality reduction - Relationship between SVD and PCA. How to use SVD to perform PCA? - Cross Validated (stackexchange.com) linear algebra - Why does Andrew Ng prefer to use SVD and not EIG of covariance matrix to do PCA? - Cross Validated (stackexchange.com) The singular value decomposition of a matrix \\(A\\) is \\(A=U\\Sigma V^T\\) , where the columns of \\(V\\) are eigenvectors of \\(A^TA\\) and the diagonal entries of \\(\\Sigma\\) are the square roots of its eigenvalues, i.e. \\(\\sigma_{ii}=\\sqrt{\\lambda_i(A^TA)}\\) . Multiplying a matrix by a scalar leaves the eigenvectors unchanged and multiplies every eigenvalue by the same scalar. \\(\\mathbf{C} \\text{(covariance matrix)}=\\mathbf{X}^{\\top} \\mathbf{X} /(n-1)\\) Wikipadia on symmetric matrix: Diagonalizable symmetric covariance matrix C can be decomposed as \\(\\mathbf{C}=\\mathbf{V} \\mathbf{L} \\mathbf{V}^{\\top} (1)\\) If we now perform singular value decomposition of X, we obtain a decomposition \\(\\mathbf{X}=\\mathbf{U S} \\mathbf{V}^{\\top}\\) \\(\\mathbf{C}=\\mathbf{V S} \\mathbf{U}^{\\top} \\mathbf{U S V}^{\\top} /(n-1)=\\mathbf{V} \\frac{\\mathbf{S}^{2}}{n-1} \\mathbf{V}^{\\top} (2)\\) Compare equation 1 vs. equation 2 , they are the same. So there are 2 ways to get eigenvectors of covariance matrix equation 1 SVD of covariance matrix to get eigenvectors of covariance matrix \\(C\\) equation 2 SVD of raw matrix \\(X\\) is the same as SVD of covariance matrix \\(C\\) ( \\(L = S_{raw}^2/(n-1)\\) ) def pca(data): data = data.T m,n = data.shape data_trans = 1/np.sqrt(n-1) * data.T u, s, vh = np.linalg.svd(data_trans) print(s**2/np.sum(s**2)) # explained_variance_ratio_ print(vh) print(data.T @ vh) pca(data) # [0.83333333 0.16666667] # [[ 0.70710678 0.70710678] # [ 0.70710678 -0.70710678]] # [[-2.12132034 0.70710678] # [-0.70710678 -0.70710678] # [ 0. 0. ] # [ 2.12132034 0.70710678] # [ 0.70710678 -0.70710678]] The art of using t-SNE for single-cell transcriptomics | Nature Communications t-SNE Initialization Options (jlmelville.github.io) Initialization is critical for preserving global data structure in both t-SNE and UMAP | Nature Biotechnology dkobak/tsne-umap-init: Initialization is critical for preserving global data structure in both t-SNE and UMAP (github.com) # What happens if we do standardization before PCA import numpy as np # preprocessing librarySizes = np.array(np.sum(batch003['counts'], axis=1)) # librarySizes = np.sum(batch003['counts'], axis=1).reshape(-1, 1) X = np.log2(batch003['counts'][:, importantGenes_idx] / librarySizes * np.median(librarySizes) + 1) X = np.array(X) X = X - X.mean(axis=0) X = X / X.std(axis=0) # pca to speed up algorithm U,s,V = np.linalg.svd(X, full_matrices=False) U[:, np.sum(V,axis=1)<0] *= -1 X = np.dot(U, np.diag(s)) X = X[:, np.argsort(s)[::-1]][:,:50] X = X / np.max(np.abs(X)) # pca-based tsne PCAinit = X[:,:2] / np.std(X[:,0]) * .0001 Z = fast_tsne(X, perplexity=30, initialization=PCAinit) Don't abuse PCA The problem of PCA is that it only works well when the first 2 principal components account for most of the variation in the data UMAP Dimension Reduction, Main Ideas!!! - YouTube It's a bad idea to use PCA to prevent overfitting - use PCA wisely Don't abuse it and use it only when raw data (original features) doesn't work When to use PCA:","title":"Week 6 8"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#metrics-in-machine-learning-system-design","text":"Bias and variance: Diagnosing Bias vs. Variance | Coursera \\(J_{CV}\\) denotes cross validation error which is the same as validation error \\(J_V\\) Precision vs. Recall Note that, in convention, \\(y=1\\) usually presents rare class OR class we are more interested in e.g. patients with cancer we want to detect PR curve Note the PR curve gives precision and recall at all thresholds - high threshold gives high precision but low recall, low threshold gives low precision but high recall (this is what we want in the cancer example) How to choose a good threshold? A: use F-score, \\(F_1 Score = 2\\frac{PR}{P+R}\\) , in mathematics, this is called the harmonic mean of precision and sensitivity.","title":"Metrics in Machine Learning System Design"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#svm","text":"The intuition of designing loss function for logistic regression should gives some clue how we design it in SVM: I think I mentioned it previously: The whole cost function consists of original cost function and an additional regularization term. So the larger \\(\\lambda\\) means the higher weight it has in the final cost function, meaning the regularization has higher amount of impact. Thus, in extreme case with very large regularization, the function can be underfitted. Likewise, the larger \\(C = 1/\\lambda\\) which is the parameter for original cost function (non regularization part) means less impact of the regularization. Thus, in extreme case with no regularization, the function is trying to do perfect job even it comes with overfitting. machine learning - What is the influence of C in SVMs with linear kernel? - Cross Validated (stackexchange.com) So here, Andrew is trying to illustrate the case that SVM is trying to separate two classes without any errors: Udacity provides another understanding of the optimization problem. For two vectors \\(x_{1},x_{2}\\) that are on two support vectors we have: \\[ \\begin{array}{l}{\\omega^{\\top} x_{1}+b=1} \\\\ {\\omega^{\\top} x_{2}+b=-1}\\end{array} \\] If you subtract them, the distance between planes (i.e. margin) can be presented as \\[ \\frac{\\omega^{T}\\left(x_{1}-x_{2}\\right)}{\\|\\omega\\|}=\\frac{2}{\\|\\omega\\|} \\] s.t. for two classifications/labels \\(y_i = 1/-1\\) , \\(y_i*(w^Tx_i+b) \\geq 1\\) Why? Because \\(w^T\\) is the direction vertical to the hyperplane so the left part means exactly the projected distance of the vector \\(x_{1} - x_{2}\\) on unit vector \\(w^T / \\|w^T\\|\\) Why \\(w^T\\) is vertical to the hyperplane? Imagine two points on the hyperplane \\(w^Tx+b =0\\) \\(w^Tx_1+b = w^Tx_2+b = 0\\) \\(w^T(x_1-x_2)=0\\) \\(x_{1} - x_{2}\\) is the vector on the hyperplane and thus \\(w ^T\\) is the normal vector. bam!!! Maximizing the margin equals to minimizing the reciprocal along with monotone \\[ \\begin{array}{l}{\\max \\frac{2}{\\| w \\|}} \\\\ {\\min 1 / 2\\|w\\|^{2}}\\end{array} \\]","title":"SVM"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#polynomial-kernel","text":"I personally prefer explanations from Josh here: Video Index - StatQuest!!! (there are three videos) Support Vector Machines Part 2: The Polynomial Kernel (Part 2 of 3) - YouTube Polynomial kernel formula: \\[ (a * b + r)^d \\\\ a.b: \\text{a and b refer to two different data in the dataset (1 dimension)} \\\\ r: \\text{r determines the coefficient of polynomial} \\\\ d: \\text{d determines the degree of polynomial} \\] Note \\(r\\) and \\(d\\) are determined through cross validation, here we set \\(r = 1\\) and \\(d = 2\\) for illustration \\[ (a*b+1)^2 = (\\sqrt{2}a,a^2,1)\\cdot(\\sqrt{2}b,b^2,1) \\] It means we create two new features (constant 1 is the same so we ignore it): \\(\\sqrt{2}*x, x^2\\) to help differentiate original \\(a\\) and \\(b\\) where there is only one feature \\(x\\) In practice, we can directly plug values into the kernel where the output represents the relationship between two data in 2-dimensions without actually transform the data to 2-Dimensions.","title":"Polynomial Kernel"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#radial-kernel","text":"Support Vector Machines Part 3: The Radial (RBF) Kernel (Part 3 of 3) - YouTube RBF formula: \\[ e^{-\\gamma(a - b)^2} \\\\ \\gamma: \\text{determines the scale of influence two points have on each other} \\] Go through the video and you will find <mark> how RBF kernel defines the relationship between two points in infinite-dimensions is genius! </mark> Let's set \\(\\gamma=1/2\\) for illustration: \\(e^{-1/2(a - b)^2} = e^{-1/2(a^2+b^2)} e^{ab}\\) With Taylor Series Expansion: \\(e^x = f(a)+\\frac {f'(a)}{1!} (x-a)+ \\frac{f''(a)}{2!} (x-a)^2+\\frac{f'''(a)}{3!}(x-a)^3+ \\cdots\\) We set \\(a = 0\\) for series above and replace x with ab So, for original coordinates that has only one dimension \\(x\\) , the new coordinates are of infinite number of dimensions. Note, like mentioned before, we are not actually projecting data into infinite number of dimensions and then try to figure out how to separate them in SVM. Instead, we plug the value into kernel function where the output (a single number) represents the relationship between two data points.","title":"Radial Kernel"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#k-means","text":"from statistics import mean def cal_dist(p1,p2): return sum((a-b)**2 for a,b in zip(p1,p2))**0.5 def kmeans(dataset,k,threshold=1e-10,n_iter=50): \"\"\" Args: dataset : array k : number of K threshold : centroids merge threshold e.g. dist(prev_centroid,new_centroid) <= thrshold == same_centroid n_iter : maximum number of iterations Returns: location of final centroids \"\"\" dataset = [tuple(i) for i in dataset] prev_centroids = {i: [] for i in dataset[:k]} for n in range(1, n_iter + 1): for data in dataset: # step1: assign datapoint to its closest centroid closest_centroid = min(prev_centroids,key=lambda x:cal_dist(data,x)) prev_centroids[closest_centroid].append(data) cur_centroids = [] for centroid, cluster in prev_centroids.items(): # calculate new centroid mean_x,mean_y = mean(c[0] for c in cluster),mean(c[1] for c in cluster) cur_centroids.append((mean_x,mean_y)) # return if all new centroids are close (dist <= threshold) to old centroids if all(cal_dist(p,q) <= threshold for p,q in zip(prev_centroids.keys(),cur_centroids)): print(f'Optimization finished with {n} iterations') for k,v in prev_centroids.items(): print(k,':',v) break # update prev_centroids with cur_centroids prev_centroids = {centroid:[] for centroid in cur_centroids} return cur_centroids # A = [(random.randint(0,10),random.randint(0,10)) for _ in range(1000)] A = [[1,1],[2,1],[4,3],[5,4],[100,10],[20,20]] print(kmeans(A,2))","title":"K-Means"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#elbow-method","text":"When trying to determine number of K in K-Means, we can use \"elbow\" method. Similarly in PCA, when trying to determine the number of principal component for further analysis, we can use this method in scree plot. From my previous manuscript: However, inferentially determining the number of PCs remains a difficult task and there is no single approach. To address this problem, 10-fold cross-validation was used. The first 50 PCs were used in cross-validation while always keeping other covariates. The overall root-mean-square error (RMSE) and R-squared would be our metrics when evaluating model performance. Note that the smallest RMSE or the largest R-squared doesn\u2019t significantly indicate the best number of PCs to use in the model. Overfitting may occur and affect the predictive accuracy of the regression model with a new dataset. As such, a nested model search was performed using F-tests on groupings of variables. Thus, we compared nested models without any principal components of daily activity and models with different numbers of components. GraphPad Prism 9 Curve Fitting Guide - How the F test works to compare models","title":"Elbow Method"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#pca","text":"The mathematics to inferentially understand why calculating eigen decomposition of covariance matrix == getting the principal component that minimize the distance to PC/maximize the variance of projections is included in my github repo.","title":"PCA"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#eigen-decomposition-of-covariance-matrix","text":"import numpy as np def pca(data): # scale data based on mean of features feature_mean = np.mean(data.T,axis=1) data_std = data - feature_mean covMatrix = np.cov(data_std.T) eigenValues, eigenVectors = np.linalg.eig(covMatrix) # eigenVectors: The normalized (unit \u201clength\u201d) eigenvectors, such that the column v[:,i] is the eigenvector corresponding to the eigenvalue w[i]. idx = eigenValues.argsort()[::-1] eigenValues = eigenValues[idx] eigenVectors = eigenVectors[:,idx] projections = data_std @ eigenVectors # definitions of dot product of two vectors/matrices print(eigenValues) print(eigenVectors) print(projections) # projections[0] is the projection of data_std[0] in the new coordinates x = [-1,-1,0,2,0] y = [-2,0,0,1,1] data = np.array([[i,j] for i,j in zip(x,y)]) pca(data) # [2.5 0.5] # [[ 0.70710678 -0.70710678] # [ 0.70710678 0.70710678]] # [[-2.12132034 -0.70710678] # [-0.70710678 0.70710678] # [ 0. 0. ] # [ 2.12132034 -0.70710678] # [ 0.70710678 0.70710678]]","title":"eigen-decomposition of covariance matrix"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#svd","text":"The SVD decomposition is heavily linear algebra required, please read through some main concepts here: Symmetric matrix - Wikipedia Eigendecomposition of a matrix - Wikipedia dimensionality reduction - Relationship between SVD and PCA. How to use SVD to perform PCA? - Cross Validated (stackexchange.com) linear algebra - Why does Andrew Ng prefer to use SVD and not EIG of covariance matrix to do PCA? - Cross Validated (stackexchange.com) The singular value decomposition of a matrix \\(A\\) is \\(A=U\\Sigma V^T\\) , where the columns of \\(V\\) are eigenvectors of \\(A^TA\\) and the diagonal entries of \\(\\Sigma\\) are the square roots of its eigenvalues, i.e. \\(\\sigma_{ii}=\\sqrt{\\lambda_i(A^TA)}\\) . Multiplying a matrix by a scalar leaves the eigenvectors unchanged and multiplies every eigenvalue by the same scalar. \\(\\mathbf{C} \\text{(covariance matrix)}=\\mathbf{X}^{\\top} \\mathbf{X} /(n-1)\\) Wikipadia on symmetric matrix: Diagonalizable symmetric covariance matrix C can be decomposed as \\(\\mathbf{C}=\\mathbf{V} \\mathbf{L} \\mathbf{V}^{\\top} (1)\\) If we now perform singular value decomposition of X, we obtain a decomposition \\(\\mathbf{X}=\\mathbf{U S} \\mathbf{V}^{\\top}\\) \\(\\mathbf{C}=\\mathbf{V S} \\mathbf{U}^{\\top} \\mathbf{U S V}^{\\top} /(n-1)=\\mathbf{V} \\frac{\\mathbf{S}^{2}}{n-1} \\mathbf{V}^{\\top} (2)\\) Compare equation 1 vs. equation 2 , they are the same. So there are 2 ways to get eigenvectors of covariance matrix equation 1 SVD of covariance matrix to get eigenvectors of covariance matrix \\(C\\) equation 2 SVD of raw matrix \\(X\\) is the same as SVD of covariance matrix \\(C\\) ( \\(L = S_{raw}^2/(n-1)\\) ) def pca(data): data = data.T m,n = data.shape data_trans = 1/np.sqrt(n-1) * data.T u, s, vh = np.linalg.svd(data_trans) print(s**2/np.sum(s**2)) # explained_variance_ratio_ print(vh) print(data.T @ vh) pca(data) # [0.83333333 0.16666667] # [[ 0.70710678 0.70710678] # [ 0.70710678 -0.70710678]] # [[-2.12132034 0.70710678] # [-0.70710678 -0.70710678] # [ 0. 0. ] # [ 2.12132034 0.70710678] # [ 0.70710678 -0.70710678]]","title":"SVD"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#the-art-of-using-t-sne-for-single-cell-transcriptomics-nature-communications","text":"t-SNE Initialization Options (jlmelville.github.io) Initialization is critical for preserving global data structure in both t-SNE and UMAP | Nature Biotechnology dkobak/tsne-umap-init: Initialization is critical for preserving global data structure in both t-SNE and UMAP (github.com) # What happens if we do standardization before PCA import numpy as np # preprocessing librarySizes = np.array(np.sum(batch003['counts'], axis=1)) # librarySizes = np.sum(batch003['counts'], axis=1).reshape(-1, 1) X = np.log2(batch003['counts'][:, importantGenes_idx] / librarySizes * np.median(librarySizes) + 1) X = np.array(X) X = X - X.mean(axis=0) X = X / X.std(axis=0) # pca to speed up algorithm U,s,V = np.linalg.svd(X, full_matrices=False) U[:, np.sum(V,axis=1)<0] *= -1 X = np.dot(U, np.diag(s)) X = X[:, np.argsort(s)[::-1]][:,:50] X = X / np.max(np.abs(X)) # pca-based tsne PCAinit = X[:,:2] / np.std(X[:,0]) * .0001 Z = fast_tsne(X, perplexity=30, initialization=PCAinit)","title":"The art of using t-SNE for single-cell transcriptomics | Nature Communications"},{"location":"Stanford_Machine_Learning_Coursera/Week_6-8/#dont-abuse-pca","text":"The problem of PCA is that it only works well when the first 2 principal components account for most of the variation in the data UMAP Dimension Reduction, Main Ideas!!! - YouTube It's a bad idea to use PCA to prevent overfitting - use PCA wisely Don't abuse it and use it only when raw data (original features) doesn't work When to use PCA:","title":"Don't abuse PCA"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/","text":"Application-level and advanced high-level topics in Machine Learning Anomaly Detection System Gaussian Distribution Anomaly detection uses gaussian distribution - probability density function formula The hypothesis model uses density estimation, the product of all density functions \\(p(x)\\) , to detect frauds The concept here is deeply incorporated into another concept likelihood - the anonymous data points usually have low likelihood Likewise, anonymous data points usually have low value in density estimation function Feature engineering Instead of \\(x_{new} = \\frac{x-\\mu}{\\sigma}\\) , one can do log transform or change the degree of feature to form gaussian-like shape to make model happy e.g. \\(x_{new} = log(x + 1)\\) , or \\(x_{new} = x^{0.2}\\) Sometimes \\(p(x)\\) is not that comparable (say, both large) for normal and anonymous data. To solve this problem, we can define new features e.g. \\(x_3 = {x_1}^2/x_2\\) which can help capture unusually large or small values (outliers) Multivariate Gaussian Distribution Motivation: what if normal data points cluster don't follow standard gaussian distribution shape - normal data are not within perfect circle but oval instead even when features are normalized (when feature engineering can't help a lot). e.g. normal (red) vs. anonymous (green) - we can't draw circle (pink) bound to separate two classifications but need to draw oval (blue) bound below Model hypothesis: Similar to normal gaussian distribution step, to detect anonymous data using multivariate gaussian distribution, we plug data into model and use a threshold \\(\\epsilon\\) Multivariate vs. single variate So multivariate gaussian model basically is a more general form with flexible covariance matrix \\(\\sum\\) , where original normal gaussian model requires \\(\\sum\\) to be diagonal Large Scale Machine Learning Different names for different kind of gradient descents batch/mini-batch/stochastic gradient descent Online Learning In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Photo OCR","title":"Week 9 11"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#anomaly-detection-system","text":"","title":"Anomaly Detection System"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#gaussian-distribution","text":"Anomaly detection uses gaussian distribution - probability density function formula The hypothesis model uses density estimation, the product of all density functions \\(p(x)\\) , to detect frauds The concept here is deeply incorporated into another concept likelihood - the anonymous data points usually have low likelihood Likewise, anonymous data points usually have low value in density estimation function","title":"Gaussian Distribution"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#feature-engineering","text":"Instead of \\(x_{new} = \\frac{x-\\mu}{\\sigma}\\) , one can do log transform or change the degree of feature to form gaussian-like shape to make model happy e.g. \\(x_{new} = log(x + 1)\\) , or \\(x_{new} = x^{0.2}\\) Sometimes \\(p(x)\\) is not that comparable (say, both large) for normal and anonymous data. To solve this problem, we can define new features e.g. \\(x_3 = {x_1}^2/x_2\\) which can help capture unusually large or small values (outliers)","title":"Feature engineering"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#multivariate-gaussian-distribution","text":"Motivation: what if normal data points cluster don't follow standard gaussian distribution shape - normal data are not within perfect circle but oval instead even when features are normalized (when feature engineering can't help a lot). e.g. normal (red) vs. anonymous (green) - we can't draw circle (pink) bound to separate two classifications but need to draw oval (blue) bound below Model hypothesis: Similar to normal gaussian distribution step, to detect anonymous data using multivariate gaussian distribution, we plug data into model and use a threshold \\(\\epsilon\\)","title":"Multivariate Gaussian Distribution"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#multivariate-vs-single-variate","text":"So multivariate gaussian model basically is a more general form with flexible covariance matrix \\(\\sum\\) , where original normal gaussian model requires \\(\\sum\\) to be diagonal","title":"Multivariate vs. single variate"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#large-scale-machine-learning","text":"Different names for different kind of gradient descents batch/mini-batch/stochastic gradient descent","title":"Large Scale Machine Learning"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#online-learning","text":"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once.","title":"Online Learning"},{"location":"Stanford_Machine_Learning_Coursera/Week_9-11/#photo-ocr","text":"","title":"Photo OCR"}]}